"""
FineTuneFlow â€” Report Generator.

Generates a Markdown training report from run metrics and configuration.
"""

from __future__ import annotations

from datetime import timedelta
from pathlib import Path
from typing import Any, Optional

from app.core.logging import get_logger

logger = get_logger(__name__)

REPORT_TEMPLATE = """# FineTuneFlow Training Report

## Project
- **Name**: {project_name}
- **Task**: {task_type}
- **Base Model**: {base_model_id}

## Dataset
- **Train examples**: {n_train}
- **Eval examples**: {n_eval}

## Training Configuration
- **Method**: {method}
- **Epochs**: {num_epochs}
- **Learning rate**: {learning_rate}
- **Batch size**: {batch_size} (x{grad_accum} gradient accumulation)
- **LoRA rank**: {lora_r}, alpha: {lora_alpha}, dropout: {lora_dropout}
- **Max sequence length**: {max_seq_length}
- **Optimizer**: {optim}
- **Scheduler**: {lr_scheduler_type}
- **GPU**: {gpu_name}

## Results
| Metric | Value |
|--------|-------|
| Final train loss | {train_loss} |
| Final eval loss | {eval_loss} |
| Perplexity | {perplexity} |
| Training time | {duration} |
| Total steps | {total_steps} |

## Training Loss Curve

Steps and loss values (for chart reconstruction):

{loss_curve_section}

## Sample Outputs

{inference_section}

---
*Generated by FineTuneFlow v0.1.0*
"""


def _fmt_duration(seconds: Optional[int]) -> str:
    if seconds is None:
        return "N/A"
    td = timedelta(seconds=seconds)
    hours, remainder = divmod(td.seconds, 3600)
    minutes, secs = divmod(remainder, 60)
    if td.days > 0:
        return f"{td.days}d {hours}h {minutes}m"
    if hours > 0:
        return f"{hours}h {minutes}m {secs}s"
    return f"{minutes}m {secs}s"


def _fmt_float(val: Any, precision: int = 6) -> str:
    if val is None:
        return "N/A"
    try:
        return f"{float(val):.{precision}f}"
    except (TypeError, ValueError):
        return str(val)


def _build_loss_curve_section(metrics: dict) -> str:
    """Build a markdown table of loss curve data."""
    train_curve = metrics.get("train_loss_curve", [])
    eval_curve = metrics.get("eval_loss_curve", [])

    if not train_curve and not eval_curve:
        return "*No loss curve data available.*"

    lines = ["| Step | Train Loss | Eval Loss |", "|------|------------|-----------|"]

    # Merge eval loss into train steps by step number
    eval_by_step = {e.get("step"): e.get("eval_loss") for e in eval_curve if e.get("step") is not None}

    for point in train_curve:
        step = point.get("step")
        if step is None:
            continue
        t_loss = _fmt_float(point.get("loss"), 6)
        e_loss = _fmt_float(eval_by_step.get(step), 6)
        lines.append(f"| {step} | {t_loss} | {e_loss} |")

    return "\n".join(lines)


def _build_inference_section(inference_samples: list[dict]) -> str:
    """Build a markdown section showing sample inference results."""
    if not inference_samples:
        return "*No inference samples available.*"

    sections = []
    for i, sample in enumerate(inference_samples, 1):
        sections.append(f"### Sample {i}")
        sections.append(f"**Instruction**: {sample.get('instruction', 'N/A')[:300]}")
        if sample.get("input"):
            sections.append(f"**Input**: {sample['input'][:300]}")
        sections.append(f"**Expected**: {sample.get('expected_output', 'N/A')[:500]}")
        sections.append(f"**Model Output**: {sample.get('model_output', 'N/A')[:500]}")
        sections.append("")

    return "\n".join(sections)


def generate_report(
    project_name: str,
    task_type: str,
    base_model_id: str,
    method: str,
    hyperparams: dict,
    metrics: dict,
    hardware_info: dict,
    num_train_examples: int,
    num_eval_examples: int,
    duration_seconds: Optional[int] = None,
    inference_samples: Optional[list[dict]] = None,
) -> str:
    """
    Generate a Markdown training report.

    Returns:
        str: The Markdown report content.
    """
    report = REPORT_TEMPLATE.format(
        project_name=project_name,
        task_type=task_type,
        base_model_id=base_model_id,
        method=f"{method.upper()} (LoRA)" if method == "lora" else f"{method.upper()} (4-bit LoRA)",
        n_train=num_train_examples,
        n_eval=num_eval_examples,
        num_epochs=hyperparams.get("num_epochs", "N/A"),
        learning_rate=hyperparams.get("learning_rate", "N/A"),
        batch_size=hyperparams.get("per_device_train_batch_size", hyperparams.get("per_device_batch_size", "N/A")),
        grad_accum=hyperparams.get("gradient_accumulation_steps", "N/A"),
        lora_r=hyperparams.get("lora_r", "N/A"),
        lora_alpha=hyperparams.get("lora_alpha", "N/A"),
        lora_dropout=hyperparams.get("lora_dropout", "N/A"),
        max_seq_length=hyperparams.get("max_seq_length", "N/A"),
        optim=hyperparams.get("optim", "paged_adamw_8bit"),
        lr_scheduler_type=hyperparams.get("lr_scheduler_type", "cosine"),
        gpu_name=hardware_info.get("gpu_name", "N/A"),
        train_loss=_fmt_float(metrics.get("train_loss"), 6),
        eval_loss=_fmt_float(metrics.get("eval_loss"), 6),
        perplexity=_fmt_float(metrics.get("perplexity"), 4),
        duration=_fmt_duration(duration_seconds),
        total_steps=metrics.get("train_steps", metrics.get("total_flos", "N/A")),
        loss_curve_section=_build_loss_curve_section(metrics),
        inference_section=_build_inference_section(inference_samples or []),
    )

    return report


def generate_report_file(
    output_path: str | Path,
    **kwargs,
) -> Path:
    """Generate and write the report to a file."""
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    content = generate_report(**kwargs)
    output_path.write_text(content, encoding="utf-8")

    logger.info("report_generator.saved", path=str(output_path))
    return output_path
